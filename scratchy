"""
Created on Thu Aug  8 12:39:27 2019

@author: sguruach

ANN from scratch - the hard way!

from https://towardsdatascience.com/neural-networks-from-scratch-easy-vs-hard-b26ddc2e89c7
and http://neuralnetworksanddeeplearning.com/chap3.html

Artificial neural network architecture:
    1. input layer = 64 neurons (input image array)
    2. hidden layer 1 = 128 neurons (arbitrary)
    3. hidden layer 2 = 128 neurons (arbitrary)
    4. output layer = 10 neurons (output one-hot array)
    5. sigmoidal activation in the two hidden layers
    6. softmax activation in the output layer
    7. cost function = cross-entropy
    
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split


def sigmoid(s):
        return 1/(1+np.exp(-s))
    
#for numerical stability, values are normalized
def softmax(s):
    exps = np.exp(s - np.max(s, axis=1, keepdims=True))
    return exps/np.sum(exps, axis=1, keepdims=True)

def sigmoid_derv(s):
    return s*(1-s)

def cross_entropy(pred, real):
    n_samples = real.shape[0]
    res = pred - real
    return res/n_samples

def error(pred, real):
    n_samples = real.shape[0]
    logp = - np.log(pred[np.arange(n_samples), real.argmax(axis=1)])
    loss = np.sum(logp)/n_samples
    return loss

#My neural network
class myNN:
    def __init__(self, x, y):
        self.x = x
        self.y = y
        neurons = 10       #number of neurons in hidden layers
        self.lr = 2       #user defined learning rate
        ip_dim = x.shape[1] #input layer size 64
        op_dim = y.shape[1] #outpur layer size 10
        self.w1 = np.random.randn(ip_dim, neurons)  #weights
        self.b1 = np.zeros((1, neurons))            #biases
        self.w2 = np.random.randn(neurons, neurons)
        self.b2 = np.zeros((1, neurons))
        self.w3 = np.random.randn(neurons, op_dim)
        self.b3 = np.zeros((1, op_dim))
    
    
    def feedforward(self):
        #computes z = (input * weights) + bias at each neuron
        #then a = sig(z) or softmax(z)
        z1 = np.dot(self.x, self.w1) + self.b1
        self.a1 = sigmoid(z1)     
        z2 = np.dot(self.a1, self.w2) + self.b2
        self.a2 = sigmoid(z2)     
        z3 = np.dot(self.a2, self.w3) + self.b3
        self.a3 = softmax(z3)
        
    def backprop(self):
        #using cross-entropy as cost function, since dealing with multi-class problem
        #where the output is a probability distribution
        #cost c = -(y * log(a3))
        #backprop update rule for w can be derived using chain rule. 
        #See reference for details: http://neuralnetworksanddeeplearning.com/chap3.html
        loss = error(self.a3, self.y)
        print('Error :', loss)
        a3_delta = cross_entropy(self.a3, self.y) #w3
        z2_delta = np.dot(a3_delta, self.w3.T)
        a2_delta = z2_delta*sigmoid_derv(self.a2) #w2
        z1_delta = np.dot(a2_delta, self.w2.T)
        a1_delta = z1_delta*sigmoid_derv(self.a1) #w1
        
        self.w3 -= self.lr * np.dot(self.a2.T, a3_delta)        
        self.b3 -= self.lr * np.sum(a3_delta, axis=0, keepdims=True)
        self.w2 -= self.lr * np.dot(self.a1.T, a2_delta)
        self.b2 -= self.lr * np.sum(a2_delta, axis=0)
        self.w1 -= self.lr * np.dot(self.x.T, a1_delta)
        self.b1 -= self.lr * np.sum(a1_delta, axis=0)
        
    def predict(self, data):
        self.x = data
        self.feedforward()
        return self.a3.argmax()
    

##Data preparation using MNIST data
dig = load_digits()
print(dig.data.shape)
onehot_target = pd.get_dummies(dig.target)
x_train, x_test, y_train, y_test = train_test_split(dig.data, onehot_target, test_size=0.1, random_state=20)
# shape of x_train : (1617, 64)
# shape of y_train : (1617, 10)
plt.gray()
plt.matshow(dig.images[25])


#instantiate. N.B. x_train has been normalized by 16, while y_train has been 
#converted from dataframe (onehot_target) into array        
model = myNN(x_train/16.0, np.array(y_train))


epochs = 1500
for x in range(epochs):
    model.feedforward()
    model.backprop()
    
def get_acc(x, y):
    #get the accuracy of the training
    acc = 0
    for xx, yy in zip(x,y):
        s = model.predict(xx)
        if s == np.argmax(yy):
            acc += 1
    return acc/len(x)*100
                   
print("Training accuracy : ", get_acc(x_train/16, np.array(y_train)))
print("Test accuracy : ", get_acc(x_test/16, np.array(y_test)))        
